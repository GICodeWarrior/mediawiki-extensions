## System config
console.encoding = "UTF-8"

## language handling

# treat "commons" as a language code
languages.commonsAsLanguage = false

# treat "simple" as a language code
languages.simpleAsLanguage = true

## RDF Export Config
# datase URI qualifier - should uniquely identify the entity creating
# the datasets (that is, YOU). This enusres unique dataset URIs for 
# datasets created by different people.
# The default is "*" which means "unknown, don't use this publically".
rdf.dataset.qualifier = "*"

## Import Driver
# Run import in a thread separate from the one reading and parsing the dump.
# Disabling this by the queue size to 0 will slightly reduce overhead on single-core systems;
# Using a queue will not have as big an impact if unzippers (bunzip/gunzip) are used
#dumpdriver.pageImportQueue = 8
dumpdriver.pageImportQueue = 0

# external unzippers - may boost performance, especially 
# on multi-core systems. The name of the file to
# unzip will be appended to the command given here. Spaces
# before the last / are taken to be part of the path, spaces
# after the last / separate parameters.
input.externalBunzip = null
input.externalGunzip = null
#input.externalBunzip = "/bin/bunzip2 -c"
#input.externalGunzip = "/bin/gunzip -c"

### Importer Output and Persistance ############
importer.progressInterval  = 1000
importer.safepointInterval = 30000 
#importer.safepointInterval = 1000 

### Database Performance #######################
#dbstore.backgroundFlushQueue = 4
dbstore.backgroundFlushQueue = 0
dbstore.useEntityBuffer = false
dbstore.useRelationBuffer = false
#dbstore.useEntityBuffer = true
#dbstore.useRelationBuffer = true
#dbstore.insertionBufferFactor = 16
dbstore.insertionBufferFactor = 64
#dbstore.engine = "MyISAM"
#dbstore.engine = "InnoDB"

#sql mode - see http://dev.mysql.com/doc/refman/5.1/en/server-sql-mode.html
#dbstore.sqlMode = "STRICT_ALL_TABLES"
#dbstore.sqlMode = "STRICT_TRANS_TABLES"

#NOTE: MySQL does not support 4-byte utf-8 codes. So turn everything into binary...
dbstore.useBinaryText = true;

#maximum size of sql statement, bytes! MySQL's default: a bit below 16M	bytes (!)
#if not specified, mysql is asked for the current value.
#dbstore.maxStatementSize = 16776192; 

#chunk size to use when chunking large updates by id
#default is 100000, set to 0 to disable all chunking
dbstore.queryChunkSize = 100000
dbstore.updateChunkSize = 100000

### Property Cache Fields ###########################
#dbstore.cacheReferenceSeparator = '\u001E'
#dbstore.cacheReferenceFieldSeparator = '\u001F'
dbstore.listBlobSize = 65025

### ID manager ######################################
# NOTE: when using this, allow for 116 bytes plus the average size of names per ID entry.
#       So if you have anaverage name length of 12 and expect1million entries, 
#       allow for about 1.3 gigabyte RAM to be used for ID caching.
dbstore.idManager=false
#dbstore.auxFileDir defaults to system temp dir
#dbstore.auxFileDir="/tmp" 
dbstore.idManager.bufferSize=16384 

### CycleFinder #####################################
dbstore.CycleFinder.levelWarningThreshold=32
dbstore.CycleFinder.degreeWarningThreshold=1024
dbstore.CycleFinder.maxDepth=1024

### Database Debug Output ######################
#see java.util.logging.Level for codes to use with dbstore.logLevel
dbstore.logLevel = 720
dbstore.explainSQLThreashold = 0
#dbstore.explainSQLThreashold = 1000000
dbstore.slowSQLThreashold = 0
#dbstore.slowSQLThreashold = 10
#dbstore.slowSQLThreashold = 60
dbstore.traceSQL = false

### Custom special purpose packages #################
#wikiword.ConfigPackages=["de.brightbyte.wikiword.bibliography","de.brightbyte.wikiword.bibliography.wikis"]

