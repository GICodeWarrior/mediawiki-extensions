THE BATCH MODE: LARGE WIKI ANALYSIS MADE POSSIBLE
=================================================

In general, to use WikiTrust on a wiki wiki dump you have two choices. 

One is to install the wiki as usual, install the WikiTrust extension,
use mwdumper to load the wiki dump in the database, and use
eval_online_wiki as explained in the top-level README file.  The
advantage of this approach is that it is simple.  On the other hand,
if you have many revisions (say, more than 100,000) in the wiki, the
approach causes quite a bit of database traffic, and overall, is not
very efficient.  You should expect a speed of 20 to 60 revisions per
second, depending on the disks available to the database.

The other approach consists in analyzing the dump file first, and then
loading it into the database once the analysis is complete.  This is
what is covered in this README file. 

PREREQUISITES:
=============

See the main README file for how to build WikiTrust. 
Do a "make allopt" from the top level, as described there. 

You also need to have a wiki dump file. 

PROCESSING THE DUMP
===================

Processing the dump used to be a complex process, composed of several
steps.  To facilitate the processing, we have written a wrapper file,
util/batch_process.py, which takes care of performing all steps
optimally, using the available multi-processing capabilities of your
machine.  Consequently, all the processing is reduced to two simple
steps:

Process the dumps:
------------------

cd util
./batch_process.py --cmd_dir <path>/WikiTrust/analysis --dir <process_dir> <dump_file_name.xml.7z>

Where:

<path> is the path to WikiTrust

<process_dir> is the name of a directory that will be used.
This directory needs to be sufficiently large; as of September 2009,
the processing of the Italian Wikipedia uses about 250 GB of space.

Notes:

The command batch_process.py has many options, which allow also to do
the processing in step-by-step fashion; do 

  ./batch_process.py --help

for more information.  In particular, batch_process performs the
following phases:
 
  --do_split: splits the input file into chunks
  --do_compute_stats: compute the stat files
  --do_sort_stats: sorts the statistic files
  --do_compute_rep: computes the user reputations
  --do_compute_trust: computes text trust

These phases need to be performed in the above order. 
Processing of the Italian Wikipedia as of September 2009 takes about 1
day on an 8-CPU-core machine. 
For the English Wikipedia, it is advisable to use a cluster; for
cluster processing, you can adapt do_all_it_stats.py and
do_all_it_revs.py to your needs.

Load the data in the db:
------------------------

./load_db.sh <process_dir>/sql <logfile> | mysql -u dbuser dbname -p

You will have to type the mysql user password. 
Here, <logfile> is the name of a file where loading statistics will appear.


...AND FINALLY...
=================

Check that the WikiTrust files are executable by Apache (see the
installation instructions), and ... fire it up!  It should work.
