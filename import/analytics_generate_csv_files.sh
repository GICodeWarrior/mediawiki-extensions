#!/bin/sh

# Prepare several csv files, ready for importing into analytics database
# All generated files have _in_ in name signalling these contain data ready for importing into database
# One input record corresponds to one database record

ulimit -v 8000000

clear
cd /a/analytics


# AnalyticsPrepBinariesData.pl read counts for binaries which were generated by wikistats 
# and which reside in /a/wikistats/csv_[project code]/StatisticsPerBinariesExtension.csv
# It filters and reorganizes data and produces analytics_in_binaries.csv
# Output csv contains: project code, language, month, extension name, count

perl AnalyticsPrepBinariesData.pl -i /a/wikistats/ -o /a/analytics/

# AnalyticsPrepComscoreData.pl scans /a/analytics/comscore for newest comScore csv files (with data for last 14 months) 
# parses those csv files, adds/replaces data from these csv files into master files (containing full history)
# and generates input csv file analytics_in_comscore.csv ready for importing into database
#
# note : these csv files were manually downloaded from http://mymetrix.comscore.com/app/report.aspx 
# and given more descriptive names, script finds newest files based on partial name search 
#
# -r replace (default is add only)
# -i input folder, contains manually downloaded csv files from comScore (or xls files manually converted to csv) 
# -m master files with full history
# -o output csv file, with reach per region, UV's per region and UV's per top web property, ready for import into database

perl AnalyticsPrepComscoreData.pl -r -i /a/analytics/comscore -m /a/analytics -o /a/analytics

# AnalyticsPrepWikiCountsOutput.pl reads a plethora of fields from several csv files from wikistats process
# It filters and reorganizes data and produces analytics_in_wikistats.csv, ready for import into analytics database 

perl AnalyticsPrepWikiCountsOutput.pl -i /a/wikistats/ -o /a/analytics 

# analytics_in_page_views.csv is written daily as part of WikiCountsSummarizeProjectCounts.pl 
# part of (/home/ezachte/pageviews_monthly.sh job) 
# which processes hourly projectcounts files (per wiki page view totals for one hour) from http://dammit.lt/wikistats
# and generates several files on different aggregation levels
# only action here is to copy data to this folder to have everything in one place
# note: unlike folder name suggests this file contains stats for all projects

cp /a/wikistats/csv_wp/analytics_in_page_views.csv .

